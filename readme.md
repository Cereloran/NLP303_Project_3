### Assessment Task Summary

#### Context
This assessment focuses on the importance of Transformer architecture in NLP projects. According to Sebastian Ruder, pretrained language models are becoming essential tools for NLP practitioners. The assessment aims to:
1. Provide context for the ongoing updates on transformers.
2. Apply the GPT model for predictions on creative writing without training or fine-tuning.
3. Prepare students for NLP roles by comparing new methods with traditional approaches.

#### Objectives
- Gain hands-on experience with Transformer models.
- Understand the variety of models available from the Hugging Face Hub.
- Develop skills in using open-source transformers and self-directed learning.

#### Instructions
1. **Reflective Report (1500 words +/- 10%)**:
    - **Abstract**: Detail new changes and their rationale.
    - **Project Plan Deviations**: Document the final plan, tasks, and milestones, and explain deviations from the initial plan.
    - **Problem Space**: Discuss any changes in the problem area and their impact.
    - **Prior Desk Research**: Evaluate the usefulness of previous literature research and its impact on the group.
    - **Implementation Architecture Changes**: Describe any changes in architecture due to a new problem area.
    - **Model Refinement**: Explain any significant changes in the model and data.
    - **Evaluation and Results**: Describe the evaluation setup and metrics used, and discuss the results.
    - **Other Comments**: Reflect on any conflicts or tensions within the group and what could be done differently.

2. **Group Presentation (5-7 minutes)**:
    - Film a group presentation describing your experiences with the development of the NLP/Speech Recognition rapid prototype.
    - Include key artefacts from your original proposal and reflections.
    - Use Microsoft Teams, Skype, or Zoom to record the conversation.
    - Use artefacts such as the project plan, code, and architecture diagram during your presentation.

3. **Source Code in Google Colab**:
    - Ensure your code executes without problems in your notebook.

#### Referencing
- Use appropriate APA style for citing and referencing research, existing software code, existing datasets, and any other resources.

#### Group Submission Instructions
- Submit one set of files per group.
- The group project submission requires a nominated person from the group to submit two files: the reflective report and code/markdown notes.
- Name the files as follows:
  - Reflective report: `NLP303LastNameFirstNameAssessment3.pdf`
  - Notebook file: `NLP303LastNameFirstNameAssessment3.ipynb`
- Submit the files via the Assessment link in the main navigation menu in NLP303 – Natural Language Processing and Speech Recognition.
- The Learning Facilitator will provide feedback via the Grade Centre in the LMS portal. Feedback can be viewed in My Grades.

#### References
- Dickson, B. (2020). Artificial neural networks are more similar to the brain than they get credit for. TechTalks.
- Dugas, D. (n.d.). The GPT-3 architecture, on a napkin. How deep is the machine? The Artificial Curiosity Series.
- Fedus, W., Zoph, B., & Shazeer, N. (2021). Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961.
- Iyer, A. (2021). GPT-3’s free alternative GPT-Neo is something to be excited about. VentureBeat.
- Vaswani, A., et al. (2017). Attention is all you need. Advances in Neural Information Processing Systems.
- Zhang, J. (2019). Basic neural units of the brain: neurons, synapses and action potential. arXiv preprint arXiv:1906.01703.
- Zhang, Y., et al. (2020). DIALOGPT: large-scale generative pre-training for conversational response generation. Proceedings of the 58th annual meeting of the Association for Computational Linguistics.